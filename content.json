{"pages":[{"title":"Resume","text":"James You Summary I am a Data Engineer primarily using Microsoft SQL Server with L2 Political. I am an expert in T-SQL, and I have experience in Python, C#, AWS, PowerBI, and Powershell. Skills Proficient in SQL Server Experience in Python, C#, Powershell, AWS, PowerBI Professional Experience L2Political SQL Developer/Data Engineer Aug 2014 – present Use SQL to standardize, develop, and process raw data for a variety of clients. Adapted and run manual ETL processes. Developed/maintained stored procedures and schemas. Used SQL Server Job System to run weekly automated backups on specific SQL Databases Automated ETL process related to periodic downloads from vendors Sped up ETL Process by dropping unnecessary indexes/constraints Changed SQL Server settings for better performance ie. MAXDOP, Backup Compression Fix Plan Regression issues after upgrading from SQL Server 2014 to SQL Server 2017 Rewrote Code to take advantage of new SQL Server 2017 features Developed applications/GUI's in C# for users to interface with SQL Server Created automatically updating Google Spreadsheets in Python to share internal info Created a production model of RecordLinkage Algorithm in SQL Created a Naïve Bayes Classifier of Ethnicity in SQL Set up AWS Glacier cloud backup system along with EC2 server Built a PowerBI Report to show data L2Political Operations March 2014 – Aug 2014 Take orders over the phone, provide excellent customer service, select specific data to clients. Send Voter Data to customers based on where they live and what elections they voted in and other selects Manipulate and modify data files in Excel/Text Editors/Access Take Phone Calls/Emails from clients about order specifications Education University of Washington , Seattle, WA BS in Applied Mathematical and Computational Sciences, GPA: 3.5/4.0, June 2010","link":"/about/index.html"},{"title":"RecordLinkage","text":"There is an art to matching two databases of individuals together on name/address/birthdate. Most of the field is relying heavily on the work of William E. Winkler, who wrote the seminal papers on probabilistic record linkage. Fellegi-Sunter came up with the idea of finding mu, the probability that matching pairs agree, and nu, the probability that two random unrelated pairs will disagree solely on chance. William Winkler figured out that you could calculate these variables by iteratively running an expectation maximization algorithm to find the values of mu and nu. There are issues with this iterative calcuation that can be solved with prior knowledge.The expectation maximization algorithm will underestimate certain mu values because most databases of individuals will have different people in the same household.This algorithm, therefore, will underestimate the mu value for first name, because it sees there are many possible matches that match on every field except first name. There are workarounds that are specific to databases of US individuals.From a practical standpoint, William Winkler found that it was best to manually set first name and last name mu values extremely close to 1 to adjust these and other weaknesses of the algorithm. He also came up with the Jaro-Winkler algorithm, which allows us to create a score for string similarity, which works extremely well for estimating mu and nu. One other thing you can do to improve these calculations is to base your nu on publicly available census/social security data.The census contains frequencies of last name for 2010, and social security contains first name frequency for every single person.With this first name data, you want to apply a multiplier to each year because of age issues. A name that was popular in 1910 is less relevant than a name that was popular in 1990. It does take a lot of computing power and time to do this, so you can speed things up with good blocking. Blocking is only analyzing data that has specific agreement patterns.For example, analyzing records that only agree on first and last name or first name and birthdate will speed up your recordlinkage algorithm by a lot. Generally, I think you want to have multiple combinations of two/three agreement fields. In general, this approach greatly reduces the number of false negatives compared to deterministic record linkage.People move, there are name formatting issues, and probabilistic record linkage does a much better job of dealing with uncertainty.It also provides a number which allows you to be more sure or less sure of a match based on the specific match between two records. References:WikipediaWinklerR package: FastlinkPython Package","link":"/politicaldata/index.html"},{"title":"TFT Dashboard","text":"Dashboard Link TFTDashboard is a project analyzing data from the video game, Teamfight Tactics. I use master+ data from the last day, filtered to the latest patch. It loads a whole bunch of match data from the Riot API into PostgreSQL using Python. The data is clustered using HDBScan and visualized using PowerBI. The Clustering isn’t perfect. There are issues with having two different compositions that should be separated and aren’t, and compositions that shouldn’t be seperated but actually are separated. Source Codeh","link":"/tft/index.html"}],"posts":[{"title":"Intro","text":"My name is James You and I am interested in data science. This website has technical overviews of how to use particular algorithms.","link":"/2020/11/28/index/"}],"tags":[],"categories":[]}